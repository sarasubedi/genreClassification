{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNryrC1gg//X9Dgr1C+ks2Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "HtKZKxtnaF3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "bmfA1rAnwq-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Initialization"
      ],
      "metadata": {
        "id": "D-hUfvtLW1zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = '../data/acousticbrainz-mediaeval_labels_part_a'\n",
        "\n",
        "data_a = pd.read_csv(datapath + 'a', delimiter='\\t')\n",
        "data_b = pd.read_csv(datapath + 'b', delimiter='\\t')\n",
        "\n",
        "data = pd.concat([data_a, data_b], ignore_index=True)\n",
        "\n",
        "# labels the songs that are rnb as is_rnb\n",
        "data['is_rnb'] = data.filter(like='genre').apply(lambda x: x.astype(str).str.contains(r'R&B|rnb|r&b_soul|r\\'n\\'b', case=False, na=False)).any(axis=1).astype(int)\n",
        "\n",
        "data.keys()"
      ],
      "metadata": {
        "id": "olVunS0JL9b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only need the lables for if it is rnb and song identifier\n",
        "data_labeled = data[['recordingmbid', 'is_rnb']]\n",
        "print(data_labeled['is_rnb'].value_counts())"
      ],
      "metadata": {
        "id": "bVkAmy6GMawI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "hNFPgz7jcoAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction\n",
        "Extraction process from the Load_data notebook. For more details, please look there."
      ],
      "metadata": {
        "id": "G-f-yVD_MhT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_json(data):\n",
        "    try:\n",
        "        features = []\n",
        "\n",
        "        # Timbre\n",
        "        features += data['lowlevel']['mfcc']['mean']\n",
        "        features += data['lowlevel']['gfcc']['mean']\n",
        "        features.append(data['lowlevel']['hfc']['mean'])\n",
        "\n",
        "        # Tonal - Harmony\n",
        "        features.append(data['tonal']['chords_changes_rate'])\n",
        "\n",
        "        # Tonal - Scale (major=0, minor=1)\n",
        "        scale = data['tonal'].get('key_scale', 'major')\n",
        "        features.append(1 if scale == 'minor' else 0)\n",
        "\n",
        "        # Tonal - Pitch salience & dissonance\n",
        "        features.append(data['lowlevel']['pitch_salience']['mean'])\n",
        "        features.append(data['lowlevel']['dissonance']['mean'])\n",
        "\n",
        "        # Rhythm\n",
        "        features.append(data['rhythm']['bpm'])\n",
        "        features.append(data['rhythm']['onset_rate'])\n",
        "\n",
        "        # Spectrum\n",
        "        features.append(data['lowlevel']['spectral_centroid']['mean'])\n",
        "        features.append(data['lowlevel']['spectral_complexity']['mean'])\n",
        "        features.append(data['lowlevel']['spectral_rolloff']['mean'])\n",
        "        features.append(data['lowlevel']['spectral_flux']['mean'])\n",
        "        features.append(data['lowlevel']['zerocrossingrate']['mean'])\n",
        "\n",
        "        # Spectral contrast (6D, not contrast_coeffs)\n",
        "        features += data['lowlevel']['spectral_contrast_coeffs']['mean']\n",
        "\n",
        "        # Dynamics\n",
        "        features.append(data['lowlevel']['average_loudness'])\n",
        "        features.append(data['lowlevel']['dynamic_complexity'])\n",
        "\n",
        "        # Rhythm extension\n",
        "        features.append(data['rhythm']['beats_loudness']['mean'])\n",
        "\n",
        "        # Energy band shape\n",
        "        features.append(data['lowlevel']['spectral_energyband_low']['mean'])\n",
        "        features.append(data['lowlevel']['spectral_energyband_high']['mean'])\n",
        "\n",
        "        # Harmonic structure\n",
        "        features.append(data['tonal']['hpcp_entropy']['mean'])\n",
        "        features.append(data['tonal']['key_strength'])\n",
        "\n",
        "        # Tonal energy balance\n",
        "        features.append(data['lowlevel']['spectral_entropy']['mean'])\n",
        "        features.append(data['lowlevel']['spectral_strongpeak']['mean'])\n",
        "\n",
        "        return features\n",
        "    except KeyError as e:\n",
        "        print(f\"Missing key: {e}\")\n",
        "        return None\n",
        "\n",
        "def build_feature_labels(data_sample):\n",
        "    labels = []\n",
        "\n",
        "    labels += [f\"mfcc_{i}\" for i in range(len(data_sample['lowlevel']['mfcc']['mean']))]\n",
        "    labels += [f\"gfcc_{i}\" for i in range(len(data_sample['lowlevel']['gfcc']['mean']))]\n",
        "    labels += [\"hfc\"]\n",
        "    labels += [\"chords_changes_rate\"]\n",
        "\n",
        "    labels += [\"key_scale\"]\n",
        "    labels += [\"pitch_salience\"]\n",
        "    labels += [\"dissonance\"]\n",
        "    labels += [\"bpm\", \"onset_rate\"]\n",
        "    labels += [\"spectral_centroid\", \"spectral_complexity\", \"spectral_rolloff\", \"spectral_flux\", \"zerocrossingrate\"]\n",
        "    labels += [f\"spectral_contrast_{i}\" for i in range(len(data_sample['lowlevel']['spectral_contrast_coeffs']['mean']))]\n",
        "    labels += [\"average_loudness\", \"dynamic_complexity\"]\n",
        "\n",
        "    labels += [\"beats_loudness\"]\n",
        "    labels += [\"spectral_energyband_low\", \"spectral_energyband_high\"]\n",
        "    labels += [\"hpcp_entropy\", \"key_strength\"]\n",
        "    labels += [\"spectral_entropy\", \"spectral_strongpeak\"]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def process_dataset(root_folder):\n",
        "    all_features = []\n",
        "    file_ids = []\n",
        "    labels_initialized = False\n",
        "    feature_labels = []\n",
        "\n",
        "    for subdir, _, files in os.walk(root_folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\".json\"):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                try:\n",
        "                    with open(file_path, \"r\") as f:\n",
        "                        data = json.load(f)\n",
        "\n",
        "                    features = extract_features_from_json(data)\n",
        "                    if features is None:\n",
        "                        continue\n",
        "\n",
        "                    if not labels_initialized:\n",
        "                        feature_labels = build_feature_labels(data)\n",
        "                        labels_initialized = True\n",
        "\n",
        "                    all_features.append(features)\n",
        "                    file_ids.append(file.replace('.json', ''))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed on {file}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_features, columns=feature_labels)\n",
        "    df['recordingmbid'] = file_ids\n",
        "    return df"
      ],
      "metadata": {
        "id": "BYakLb8rMeYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract JSON features\n",
        "folder_path = '../data/acousticbrainz-mediaeval-train'\n",
        "data_features = process_dataset(folder_path)\n",
        "#data_features.head()"
      ],
      "metadata": {
        "id": "kB3Jm_hvx7R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(data_features, data_labeled, on=\"recordingmbid\", how=\"inner\")\n",
        "\n",
        "# Check results\n",
        "print(merged_df.head())\n",
        "print(\"Shape:\", merged_df.shape)\n",
        "print(\"Label value counts:\\n\", merged_df['is_rnb'].value_counts())\n",
        "\n",
        "merged_df.to_csv(\"tmp/rnb_features_labeled.csv\", index=False)"
      ],
      "metadata": {
        "id": "JVKbTKF8N1YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"tmp/rnb_features_labeled.csv\")\n",
        "\n",
        "# load data for training\n",
        "train_data = df.drop(columns=['recordingmbid', 'is_rnb'])\n",
        "train_labels = df['is_rnb']"
      ],
      "metadata": {
        "id": "LIrFfml9N8Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_data)\n",
        "rnb = [i for i, label in enumerate(train_labels) if label == 1]\n",
        "print(rnb)"
      ],
      "metadata": {
        "id": "8tjenrx4cx8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histogram Creation"
      ],
      "metadata": {
        "id": "sO833f13c0oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_data = [[] for i in range(len(train_data['mfcc_0']))]\n",
        "\n",
        "for column in train_data:\n",
        "    for i in range(len(train_data[column])):\n",
        "        histogram_data[i].append(float(train_data.at[i, column]))"
      ],
      "metadata": {
        "id": "qJTJyAtAc1-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(histogram_data[0])\n",
        "\n",
        "plt.hist(histogram_data[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ttrc-llc3xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating and saving all the histograms will take a fairly long amount of time. If possible, I'd recommend breaking the data down into smaller portions to avoid any errors."
      ],
      "metadata": {
        "id": "3Wzxp6oxOYJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = os.path.join(\"CNN_data\", \"histograms\")\n",
        "\n",
        "# number of histograms processed. I recommend stopping and updating this every 30 minutes\n",
        "num_histograms = 0\n",
        "\n",
        "# total_histograms = 111048\n",
        "total_histograms = len(train_data['mfcc_0'])\n",
        "\n",
        "for i in range(num_histograms, total_histograms):\n",
        "    plt.figure()\n",
        "    histogram = plt.hist(histogram_data[i])\n",
        "    filename = str(f\"histo{i}.png\")\n",
        "    if i in rnb:\n",
        "        save_label = os.path.join(save_path, \"rnb\")\n",
        "    else:\n",
        "        save_label = os.path.join(save_path, \"other\")\n",
        "    save_file = os.path.join(save_label, filename)\n",
        "    plt.savefig(save_file)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "r8kBN5FRc7At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "2vdNsobMpnac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition\n",
        "CNN_model = models.Sequential([\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64),\n",
        "    layers.Dense(2)\n",
        "])"
      ],
      "metadata": {
        "id": "aiuevWzKDubC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model compilation\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "CNN_model.compile(optimizer='adam', metrics=['accuracy'], loss=loss)"
      ],
      "metadata": {
        "id": "51xB-_oOEkYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Training"
      ],
      "metadata": {
        "id": "h0pry2WROiR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = os.path.join(\"CNN_data\", \"histograms\")\n",
        "\n",
        "histograms = tf.keras.utils.image_dataset_from_directory(file_path,\n",
        "                                                           color_mode=\"rgb\",\n",
        "                                                           label_mode=\"int\",\n",
        "                                                           image_size=(256,256),\n",
        "                                                           batch_size=100)"
      ],
      "metadata": {
        "id": "i_r9Z1jIOlDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model.fit(histograms, epochs=5)"
      ],
      "metadata": {
        "id": "H_kqfZNIOnA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model summary\n",
        "CNN_model.summary()"
      ],
      "metadata": {
        "id": "HmITAYmqFZMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Load Model"
      ],
      "metadata": {
        "id": "9kZK6ZnCOqpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model.save(\"CNN_data/CNN_model.keras\")"
      ],
      "metadata": {
        "id": "cKtN8v9EOsr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model = tf.keras.models.load_model(\"CNN_data/CNN_model.keras\")"
      ],
      "metadata": {
        "id": "OJ-XpyNbOujP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "EzmKDW6aVoxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = CNN_model.evaluate(histograms)\n",
        "print(f'Test accuracy: {test_acc:.2f}')"
      ],
      "metadata": {
        "id": "DoZr3mrEVudD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Song Prediction"
      ],
      "metadata": {
        "id": "ZLf5WNzSV6dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction histogram\n",
        "# change to desired test histogram\n",
        "test_hist_path = \"CNN_data/histograms/other/histo1.png\"\n",
        "\n",
        "img = tf.keras.utils.load_img(test_hist_path, target_size=(256, 256))\n",
        "test_hist = tf.keras.utils.img_to_array(img)\n",
        "test_hist = np.expand_dims(test_hist, axis=0)\n",
        "\n",
        "prediction = CNN_model.predict(test_hist)\n",
        "prediction = np.argmax(prediction)\n",
        "print(f'Predicted genre: {prediction}')"
      ],
      "metadata": {
        "id": "n5xFJulOWGap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}